{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_VOZLv6d3KQ-"
      },
      "outputs": [],
      "source": [
        "#if your data is on goolge drive or you want to copy your results there mount it\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJC-0GitoJEh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a512b20b-c293-4711-d9fd-2fb31f7a6ce2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jun 13 08:25:16 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# https://github.com/yashbhalgat/HashNeRF-pytorch\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lT_DEy7Q3FMc"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijgdl-TUcxnd",
        "outputId": "058aa74c-aa0a-4395-e250-76b0640e5858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'torch-ngp'...\n",
            "remote: Enumerating objects: 1413, done.\u001b[K\n",
            "remote: Counting objects: 100% (162/162), done.\u001b[K\n",
            "remote: Compressing objects: 100% (66/66), done.\u001b[K\n",
            "remote: Total 1413 (delta 105), reused 141 (delta 95), pack-reused 1251\u001b[K\n",
            "Receiving objects: 100% (1413/1413), 579.61 KiB | 7.53 MiB/s, done.\n",
            "Resolving deltas: 100% (938/938), done.\n",
            "Submodule 'ffmlp/dependencies/cutlass' (https://github.com/NVIDIA/cutlass.git) registered for path 'ffmlp/dependencies/cutlass'\n",
            "Cloning into '/content/torch-ngp/ffmlp/dependencies/cutlass'...\n",
            "remote: Enumerating objects: 15745, done.        \n",
            "remote: Counting objects: 100% (4213/4213), done.        \n",
            "remote: Compressing objects: 100% (846/846), done.        \n",
            "remote: Total 15745 (delta 3532), reused 3823 (delta 3328), pack-reused 11532        \n",
            "Receiving objects: 100% (15745/15745), 19.02 MiB | 1.69 MiB/s, done.\n",
            "Resolving deltas: 100% (11512/11512), done.\n",
            "Submodule path 'ffmlp/dependencies/cutlass': checked out 'cd39c75e250ef1c3dd11cd59dca4e53efe7d5ea8'\n",
            "/content/torch-ngp\n"
          ]
        }
      ],
      "source": [
        "!git clone --recursive https://github.com/ashawkey/torch-ngp.git\n",
        "%cd torch-ngp "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TRfrshaCdpY8"
      },
      "outputs": [],
      "source": [
        "#add dependencies\n",
        "!pip3 install -r requirements.txt\n",
        "%cd raymarching\n",
        "!python setup.py build_ext --inplace # build ext only, do not install (only can be used in the parent directory)\n",
        "!pip install ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCW1QLG9UFpJ",
        "outputId": "588e1153-fe23-45dc-96f0-3a80ff66eda4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Access denied with the following error:\n",
            "\n",
            " \tCannot retrieve the public link of the file. You may need to change\n",
            "\tthe permission to 'Anyone with the link', or have had many accesses. \n",
            "\n",
            "You may still be able to access the file from the browser:\n",
            "\n",
            "\t https://drive.google.com/uc?id=https://drive.google.com/drive/folders/1JDdLGDruGNXWnM1eqY1FNL9PlStjaKWi \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# data https://drive.google.com/drive/folders/1JDdLGDruGNXWnM1eqY1FNL9PlStjaKWi\n",
        "!gdown --id https://drive.google.com/drive/folders/1JDdLGDruGNXWnM1eqY1FNL9PlStjaKWi\n",
        "#!mv /content/HashNeRF-pytorch/1JDdLGDruGNXWnM1eqY1FNL9PlStjaKWi /content/HashNeRF-pytorch/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aijHZB0zJwWB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98d80d0e-0267-4ec4-b396-5820f924868d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(H=1080, O=False, W=1920, bg_radius=-1, bound=2, ckpt='latest', clip_text='', color_space='srgb', cuda_ray=False, density_thresh=0.01, dt_gamma=0.0078125, error_map=False, ff=False, fovy=50, fp16=False, gui=False, iters=40000, lr=0.01, max_ray_batch=4096, max_spp=64, min_near=0.2, mode='colmap', num_rays=4096, num_steps=512, path='data/fox', preload=False, radius=5, rand_pose=-1, scale=0.33, seed=0, tcnn=False, test=False, upsample_steps=0, workspace='trial_nerf')\n"
          ]
        }
      ],
      "source": [
        "scene=\"configs/chair.txt\"\n",
        "screenshot_dir=\"/content/drive/MyDrisve/ColabNERF/fox/render\"\n",
        "snap=\"/content/drive/MyDrive/ColabNERF/fox/snap/fox_abb8.msgpack\"\n",
        "model0=\"/content/sample_data/render.obj\" \n",
        "width=512\n",
        "# to see commandline help\n",
        "!python /content/torch-ngp/main_nerf.py data/fox --workspace trial_nerf"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "torch-ngp.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}